{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Credits** By a student for students with love"
      ],
      "metadata": {
        "id": "NEvzj-xW-0G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-requisites\n",
        "```\n",
        "Equation of line\n",
        "Derivative\n",
        "```"
      ],
      "metadata": {
        "id": "q5h8wX1iDJAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithmic Intuition\n",
        "---"
      ],
      "metadata": {
        "id": "KrKdaJ_DAuHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The essence lies in making an initial random prediction, assessing its accuracy, refining the prediction system, and then repeating this iterative process for improvement.\n",
        "\n",
        "Now the question arises, how do we make the first prediction? We use a mathematical function called hypothesis. The hypothesis function has paramters, slope and y-intercept of a line for a simplest linear regression model, intilized with random weights.\n",
        "\n",
        "Once the initial prediction is made, our objective is to iteratively optimize the weights of the parameters used in the hypothesis.\n",
        "\n",
        "Improving a process requires identifying flaws or areas for enhancement. In the context of the linear regression model, we refer to this as \"error\". The disparity between the actual output and predicted output is termed \"loss\", and the average of all losses across a dataset is referred to as \"cost\".\n",
        "\n",
        "Once we identify the need for improvement in the prediction system, we employ gradient descent. Notably, the focus is on altering the hypothesis function. Since we cannot modify the dataset, the adjustment is made to the parameters. When it comes to modifying the function, it's essential to comprehend that updating parameters results in an entirely new function. This new function exhibits distinct derivatives or slopes at every point. Each function also possesses different loss and cost values at various points. Utilizing this insight, we update the weights of the parameters to iteratively refine the model.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rbeAWX-Y8uUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "Before model intialization, coding.\n",
        "1.   Choose an hypothesis function, in our case it is standard equation of line.\n",
        "2.   Choose a cost function, we will choose MSE.\n",
        "\n",
        "At model intialization, according to the dataset features:\n",
        "1.   Intialize parameters of the hypothesis function as a vector with random values.\n",
        "\n",
        "In each training iteration\n",
        "1.   Predict according to the hypothesis\n",
        "2.   Find gradient for each of the parameter present in the hypothesis function\n",
        "---"
      ],
      "metadata": {
        "id": "cmO_zPQsH7bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "gn2GOgJ7H5g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "lZ4M0eOgTu_k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Constructor\n",
        "  Recieve and store dataset arguments\n",
        "\"\"\"\n",
        "\n",
        "class Linear_Regression():\n",
        "  def __init__(self, X, Y):\n",
        "    # Recieve and store dataset arguments\n",
        "    self.X = X  # Stores X\n",
        "    self.Y = Y  # Stores Y"
      ],
      "metadata": {
        "id": "YkmnyDC1Sc_v"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Constructor\n",
        "  For use by hypothesis function -- Initiated hypothesis paramters with random weights\n",
        "  To utilize matrix multiplication -- Added a column of 1s to X\n",
        "\n",
        "Hypothesis function\n",
        "  Calculates and stores predictions\n",
        "\"\"\"\n",
        "\n",
        "class Linear_Regression():\n",
        "  def __init__(self, X, Y):\n",
        "    # Recieve and store dataset arguments\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "    # Initiated hypothesis paramters with random weights\n",
        "    if X.ndim == 1:\n",
        "      self.n, self.m = self.X.reshape(-1, 1).shape\n",
        "    else:\n",
        "      self.n, self.m = self.X.shape\n",
        "    self.thetas = np.random.rand(self.m)\n",
        "\n",
        "    # Added a column of 1s to X\n",
        "    self.X = np.hstack((np.ones((X.shape[0], 1)), X.reshape(-1, 1)))\n",
        "\n",
        "  def hypothesis(self):\n",
        "    self.Y_pred = np.dot(self.X, self.thetas.reshape(-1, 1))\n",
        "\n",
        "  def cost(self):\n"
      ],
      "metadata": {
        "id": "BrgXd3xb87Pv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1,2])\n",
        "Y = np.array([2,4])\n",
        "Linear_Regression(X, Y).hypothesis()"
      ],
      "metadata": {
        "id": "Y5flapNwSema"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzZ4Bg9TX1K8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}